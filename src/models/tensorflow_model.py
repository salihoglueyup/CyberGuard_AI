"""
TensorFlow Deep Learning Model - CyberGuard AI
Daha yÃ¼ksek doÄŸruluk iÃ§in neural network kullanÄ±r
"""

import sys
import io

# Windows console encoding fix
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8")

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, callbacks, regularizers
from tensorflow.keras.models import Sequential, load_model
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    precision_recall_fscore_support,
    roc_auc_score,
    roc_curve,
)
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import warnings

warnings.filterwarnings("ignore")

# GPU ayarlarÄ±
physical_devices = tf.config.list_physical_devices("GPU")
if physical_devices:
    print(f"ğŸ® GPU bulundu: {len(physical_devices)} adet")
    for gpu in physical_devices:
        tf.config.experimental.set_memory_growth(gpu, True)
else:
    print("ğŸ’» CPU modunda Ã§alÄ±ÅŸÄ±yor")


class CyberThreatNeuralNetwork:
    """
    GeliÅŸmiÅŸ Deep Learning modeli - Siber tehdit tespiti

    Ã–zellikler:
    - Multi-layer neural network
    - Dropout ve BatchNormalization
    - Learning rate scheduling
    - Early stopping
    - Model checkpointing
    - TensorBoard logging
    """

    def __init__(
        self,
        input_dim: int = 8,
        hidden_layers: List[int] = [256, 128, 64, 32],
        dropout_rate: float = 0.3,
        l2_reg: float = 0.001,
        learning_rate: float = 0.001,
        activation: str = "relu",
        output_activation: str = "softmax",
    ):
        """
        Args:
            input_dim: Girdi Ã¶zellik sayÄ±sÄ±
            hidden_layers: Gizli katman nÃ¶ron sayÄ±larÄ±
            dropout_rate: Dropout oranÄ± (overfitting Ã¶nleme)
            l2_reg: L2 regularization katsayÄ±sÄ±
            learning_rate: Ã–ÄŸrenme hÄ±zÄ±
            activation: Aktivasyon fonksiyonu
            output_activation: Ã‡Ä±ktÄ± aktivasyonu
        """
        self.input_dim = input_dim
        self.hidden_layers = hidden_layers
        self.dropout_rate = dropout_rate
        self.l2_reg = l2_reg
        self.learning_rate = learning_rate
        self.activation = activation
        self.output_activation = output_activation

        self.model = None
        self.history = None
        self.num_classes = None
        self.class_names = None

        print("ğŸ§  TensorFlow Neural Network baÅŸlatÄ±ldÄ±")
        print(f"   Mimari: {hidden_layers}")
        print(f"   Dropout: {dropout_rate}")
        print(f"   L2 Reg: {l2_reg}")
        print(f"   Learning Rate: {learning_rate}")

    def build_model(self, num_classes: int) -> keras.Model:
        """
        Neural network modelini oluÅŸtur

        Args:
            num_classes: SÄ±nÄ±f sayÄ±sÄ±

        Returns:
            Keras model
        """
        self.num_classes = num_classes

        model = Sequential(name="CyberThreat_DNN")

        # Input layer
        model.add(layers.Input(shape=(self.input_dim,), name="input_features"))

        # Hidden layers ile Batch Normalization ve Dropout
        for i, units in enumerate(self.hidden_layers):
            # Dense layer
            model.add(
                layers.Dense(
                    units=units,
                    activation=self.activation,
                    kernel_regularizer=regularizers.l2(self.l2_reg),
                    name=f"dense_{i + 1}",
                )
            )

            # Batch Normalization (training stability iÃ§in)
            model.add(layers.BatchNormalization(name=f"batch_norm_{i + 1}"))

            # Dropout (overfitting Ã¶nleme)
            model.add(layers.Dropout(self.dropout_rate, name=f"dropout_{i + 1}"))

        # Output layer
        model.add(
            layers.Dense(num_classes, activation=self.output_activation, name="output")
        )

        # Model compile
        optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)

        model.compile(
            optimizer=optimizer,
            loss="sparse_categorical_crossentropy",
            metrics=["accuracy"],  # Sadece accuracy - diÄŸerleri problem Ã§Ä±karÄ±yor
        )

        self.model = model

        print("\n" + "=" * 70)
        print("ğŸ—ï¸  MODEL MÄ°MARÄ°SÄ°")
        print("=" * 70)
        model.summary()
        print("=" * 70 + "\n")

        return model

    def build_lstm_model(
        self,
        input_shape: Tuple[int, int] = (1, 78),
        num_classes: int = 15,
        conv_filters: int = 30,
        lstm_units: int = 120,
        dense_units: int = 512,
    ) -> keras.Model:
        """
        Optimized LSTM-IDS Model (Makaleye Uygun)

        Mimari: Conv1D â†’ MaxPool â†’ LSTM â†’ Dense â†’ Output

        Ref: "An optimized LSTM-based deep learning model for anomaly network intrusion detection"
        Scientific Reports (2025) 15:1554

        Args:
            input_shape: (timesteps, features)
            num_classes: SÄ±nÄ±f sayÄ±sÄ±
            conv_filters: Conv1D filter sayÄ±sÄ± (default: 30)
            lstm_units: LSTM unit sayÄ±sÄ± (default: 120)
            dense_units: Dense layer unit sayÄ±sÄ± (default: 512)

        Returns:
            Compiled Keras model
        """
        self.num_classes = num_classes

        print("ğŸ”§ Optimized LSTM-IDS modeli oluÅŸturuluyor...")

        model = Sequential(name="Optimized_LSTM_IDS")

        # 1. Conv1D Layer - Pattern Extraction
        model.add(
            layers.Conv1D(
                filters=conv_filters,
                kernel_size=5,
                padding="same",
                activation="relu",
                input_shape=input_shape,
                name="conv1d_pattern_extraction",
            )
        )

        # 2. MaxPooling - Dimensionality Reduction
        model.add(layers.MaxPooling1D(pool_size=2, name="maxpool_reduction"))

        # 3. LSTM Layer - Temporal Learning
        model.add(layers.LSTM(units=lstm_units, dropout=0.2, name="lstm_temporal"))

        # 4. Dense Layer - Feature Transformation
        model.add(
            layers.Dense(
                units=dense_units, activation="sigmoid", name="dense_transform"
            )
        )

        # 5. Output Layer - Classification
        model.add(
            layers.Dense(
                units=num_classes, activation="softmax", name="output_classification"
            )
        )

        # Compile with Adam optimizer
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss="sparse_categorical_crossentropy",
            metrics=["accuracy"],
        )

        self.model = model

        print("\n" + "=" * 70)
        print("ğŸ—ï¸  OPTIMIZED LSTM-IDS MÄ°MARÄ°SÄ°")
        print("=" * 70)
        print(f"   ğŸ“Š Conv1D: {conv_filters} filters, kernel=5, ReLU")
        print(f"   ğŸ“Š MaxPooling: pool_size=2")
        print(f"   ğŸ“Š LSTM: {lstm_units} units, dropout=0.2")
        print(f"   ğŸ“Š Dense: {dense_units} units, sigmoid")
        print(f"   ğŸ“Š Output: {num_classes} classes, softmax")
        print("=" * 70)
        model.summary()
        print("=" * 70 + "\n")

        return model

    def get_callbacks(
        self,
        model_path: str = "models/best_model.h5",
        tensorboard_dir: str = "logs/tensorboard",
        patience: int = 15,
    ) -> List[callbacks.Callback]:
        """
        Training callbacks oluÅŸtur

        Args:
            model_path: En iyi modelin kaydedileceÄŸi yer
            tensorboard_dir: TensorBoard log dizini
            patience: Early stopping iÃ§in sabÄ±r

        Returns:
            Callback listesi
        """
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        os.makedirs(tensorboard_dir, exist_ok=True)

        callback_list = [
            # Early Stopping - Validation loss artmayÄ± durdurduÄŸunda dur
            callbacks.EarlyStopping(
                monitor="val_loss",
                patience=patience,
                restore_best_weights=True,
                verbose=1,
            ),
            # Model Checkpoint - En iyi modeli kaydet
            callbacks.ModelCheckpoint(
                filepath=model_path,
                monitor="val_accuracy",
                save_best_only=True,
                mode="max",
                verbose=1,
            ),
            # Learning Rate Reduction - Plateau'da Ã¶ÄŸrenme hÄ±zÄ±nÄ± azalt
            callbacks.ReduceLROnPlateau(
                monitor="val_loss", factor=0.5, patience=5, min_lr=1e-7, verbose=1
            ),
            # CSV Logger - Metrikleri kaydet
            callbacks.CSVLogger(
                filename="logs/training_log.csv", separator=",", append=False
            ),
        ]

        return callback_list

    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray = None,
        y_val: np.ndarray = None,
        epochs: int = 100,
        batch_size: int = 32,
        class_names: List[str] = None,
        verbose: int = 1,
    ) -> Dict:
        """
        Modeli eÄŸit (Thread-safe version)

        Args:
            X_train: EÄŸitim verileri
            y_train: EÄŸitim etiketleri
            X_val: Validasyon verileri
            y_val: Validasyon etiketleri
            epochs: Epoch sayÄ±sÄ±
            batch_size: Batch boyutu
            class_names: SÄ±nÄ±f isimleri
            verbose: Loglama seviyesi

        Returns:
            Training history
        """
        self.class_names = class_names

        # Model henÃ¼z oluÅŸturulmadÄ±ysa oluÅŸtur
        if self.model is None:
            num_classes = len(np.unique(y_train))
            self.build_model(num_classes)

        print("\n" + "=" * 70)
        print("ğŸ¯ MODEL EÄÄ°TÄ°MÄ° BAÅLIYOR")
        print("=" * 70)
        print(f"ğŸ“Š EÄŸitim verileri: {X_train.shape}")
        print(f"ğŸ“Š EÄŸitim etiketleri: {y_train.shape}")
        if X_val is not None:
            print(f"ğŸ“Š Validasyon verileri: {X_val.shape}")
        print(f"âš™ï¸  Epochs: {epochs}")
        print(f"âš™ï¸  Batch size: {batch_size}")
        print("=" * 70 + "\n")

        # CRITICAL FIX: TensorFlow session'Ä± temizle
        import gc

        tf.keras.backend.clear_session()
        gc.collect()

        # Basit callbacks - threading sorunlarÄ±nÄ± Ã¶nlemek iÃ§in
        callbacks_list = []

        # Early stopping
        early_stop = tf.keras.callbacks.EarlyStopping(
            monitor="val_loss", patience=15, restore_best_weights=True, verbose=1
        )
        callbacks_list.append(early_stop)

        # Reduce LR
        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
            monitor="val_loss", factor=0.5, patience=5, min_lr=1e-7, verbose=1
        )
        callbacks_list.append(reduce_lr)

        # EÄŸitim
        start_time = datetime.now()

        try:
            # Model fit - threading safe
            self.history = self.model.fit(
                X_train,
                y_train,
                validation_data=(X_val, y_val) if X_val is not None else None,
                epochs=epochs,
                batch_size=batch_size,
                callbacks=callbacks_list,
                verbose=verbose,
                shuffle=True,
            )

        except Exception as e:
            print(f"\nâš ï¸ Training hatasÄ±: {e}")
            print("ğŸ”„ Yeniden deneniyor (basit mod)...")

            # Fallback: Callback'siz dene
            self.history = self.model.fit(
                X_train,
                y_train,
                validation_data=(X_val, y_val) if X_val is not None else None,
                epochs=epochs,
                batch_size=batch_size,
                callbacks=[],  # Callback'siz
                verbose=0,  # Sessiz mod
                shuffle=True,
            )

        end_time = datetime.now()
        training_time = (end_time - start_time).total_seconds()

        print("\n" + "=" * 70)
        print("âœ… EÄÄ°TÄ°M TAMAMLANDI!")
        print("=" * 70)
        print(f"â±ï¸  SÃ¼re: {training_time:.2f} saniye ({training_time / 60:.2f} dakika)")
        print(
            f"ğŸ“ˆ Final Train Accuracy: {self.history.history['accuracy'][-1] * 100:.2f}%"
        )
        if X_val is not None:
            print(
                f"ğŸ“ˆ Final Val Accuracy: {self.history.history['val_accuracy'][-1] * 100:.2f}%"
            )
        print("=" * 70 + "\n")

        return self.history.history

    def predict(self, X: np.ndarray, return_proba: bool = False) -> np.ndarray:
        """
        Tahmin yap

        Args:
            X: Girdi verileri
            return_proba: OlasÄ±lÄ±klarÄ± dÃ¶ndÃ¼r

        Returns:
            Tahminler veya olasÄ±lÄ±klar
        """
        predictions = self.model.predict(X, verbose=0)

        if return_proba:
            return predictions

        return np.argmax(predictions, axis=1)

    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """
        Modeli deÄŸerlendir

        Args:
            X: Test verileri
            y: Test etiketleri

        Returns:
            Dict: DeÄŸerlendirme metrikleri (loss, accuracy)
        """
        if self.model is None:
            raise ValueError("Model henÃ¼z eÄŸitilmemiÅŸ!")

        results = self.model.evaluate(X, y, verbose=0)

        # Keras model.evaluate [loss, accuracy, ...] dÃ¶ner
        metrics = {"loss": float(results[0]), "accuracy": float(results[1])}

        print(f"ğŸ“Š Evaluation Results:")
        print(f"   Loss:     {metrics['loss']:.4f}")
        print(f"   Accuracy: {metrics['accuracy']*100:.2f}%")

        return metrics

    def plot_training_history(
        self,
        save_path: str = "models/training_history.png",
        figsize: Tuple[int, int] = (15, 10),
    ):
        """
        EÄŸitim geÃ§miÅŸini gÃ¶rselleÅŸtir

        Args:
            save_path: Kaydedilecek dosya yolu
            figsize: Figure boyutu
        """
        if self.history is None:
            print("âš ï¸  HenÃ¼z eÄŸitim yapÄ±lmamÄ±ÅŸ!")
            return

        history = self.history.history

        fig, axes = plt.subplots(2, 2, figsize=figsize)
        fig.suptitle("Model Training History", fontsize=16, fontweight="bold")

        # Accuracy
        axes[0, 0].plot(history["accuracy"], label="Train", linewidth=2)
        if "val_accuracy" in history:
            axes[0, 0].plot(history["val_accuracy"], label="Validation", linewidth=2)
        axes[0, 0].set_title("Model Accuracy", fontweight="bold")
        axes[0, 0].set_xlabel("Epoch")
        axes[0, 0].set_ylabel("Accuracy")
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # Loss
        axes[0, 1].plot(history["loss"], label="Train", linewidth=2)
        if "val_loss" in history:
            axes[0, 1].plot(history["val_loss"], label="Validation", linewidth=2)
        axes[0, 1].set_title("Model Loss", fontweight="bold")
        axes[0, 1].set_xlabel("Epoch")
        axes[0, 1].set_ylabel("Loss")
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # Precision
        if "precision" in history:
            axes[1, 0].plot(history["precision"], label="Train", linewidth=2)
            if "val_precision" in history:
                axes[1, 0].plot(
                    history["val_precision"], label="Validation", linewidth=2
                )
            axes[1, 0].set_title("Model Precision", fontweight="bold")
            axes[1, 0].set_xlabel("Epoch")
            axes[1, 0].set_ylabel("Precision")
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

        # Recall
        if "recall" in history:
            axes[1, 1].plot(history["recall"], label="Train", linewidth=2)
            if "val_recall" in history:
                axes[1, 1].plot(history["val_recall"], label="Validation", linewidth=2)
            axes[1, 1].set_title("Model Recall", fontweight="bold")
            axes[1, 1].set_xlabel("Epoch")
            axes[1, 1].set_ylabel("Recall")
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"âœ… Training history kaydedildi: {save_path}")
        plt.close()

    def plot_confusion_matrix(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        save_path: str = "models/confusion_matrix.png",
        figsize: Tuple[int, int] = (10, 8),
    ):
        """
        Confusion matrix gÃ¶rselleÅŸtir

        Args:
            y_true: GerÃ§ek etiketler
            y_pred: Tahmin edilen etiketler
            save_path: Kaydedilecek dosya yolu
            figsize: Figure boyutu
        """
        cm = confusion_matrix(y_true, y_pred)

        plt.figure(figsize=figsize)
        sns.heatmap(
            cm,
            annot=True,
            fmt="d",
            cmap="Blues",
            xticklabels=self.class_names if self.class_names else range(len(cm)),
            yticklabels=self.class_names if self.class_names else range(len(cm)),
            cbar_kws={"label": "Count"},
        )
        plt.title("Confusion Matrix", fontsize=14, fontweight="bold")
        plt.xlabel("Predicted Label", fontweight="bold")
        plt.ylabel("True Label", fontweight="bold")
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"âœ… Confusion matrix kaydedildi: {save_path}")
        plt.close()

    def save(
        self, model_path: str = "models/tensorflow_model.h5", save_format: str = "h5"
    ):
        """
        Modeli kaydet

        Args:
            model_path: Model dosya yolu
            save_format: KayÄ±t formatÄ± ('h5' veya 'tf')
        """
        os.makedirs(os.path.dirname(model_path), exist_ok=True)

        # Model kaydet
        self.model.save(model_path, save_format=save_format)

        # Metadata kaydet
        metadata = {
            "input_dim": self.input_dim,
            "hidden_layers": self.hidden_layers,
            "dropout_rate": self.dropout_rate,
            "l2_reg": self.l2_reg,
            "learning_rate": self.learning_rate,
            "activation": self.activation,
            "output_activation": self.output_activation,
            "num_classes": self.num_classes,
            "class_names": self.class_names,
            "saved_at": datetime.now().isoformat(),
        }

        metadata_path = model_path.replace(".h5", "_metadata.json")
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=4)

        print(f"âœ… Model kaydedildi: {model_path}")
        print(f"âœ… Metadata kaydedildi: {metadata_path}")

    @classmethod
    def load(
        cls, model_path: str = "models/tensorflow_model.h5"
    ) -> "CyberThreatNeuralNetwork":
        """
        Modeli yÃ¼kle

        Args:
            model_path: Model dosya yolu

        Returns:
            CyberThreatNeuralNetwork instance
        """
        # Metadata yÃ¼kle
        metadata_path = model_path.replace(".h5", "_metadata.json")
        with open(metadata_path, "r") as f:
            metadata = json.load(f)

        # Instance oluÅŸtur
        instance = cls(
            input_dim=metadata["input_dim"],
            hidden_layers=metadata["hidden_layers"],
            dropout_rate=metadata["dropout_rate"],
            l2_reg=metadata["l2_reg"],
            learning_rate=metadata["learning_rate"],
            activation=metadata["activation"],
            output_activation=metadata["output_activation"],
        )

        # Modeli yÃ¼kle
        instance.model = load_model(model_path)
        instance.num_classes = metadata["num_classes"]
        instance.class_names = metadata["class_names"]

        print(f"âœ… Model yÃ¼klendi: {model_path}")
        print(f"âœ… Metadata yÃ¼klendi: {metadata_path}")

        return instance


# Ã–rnek kullanÄ±m
if __name__ == "__main__":
    print("ğŸ§ª TensorFlow Model Test")
    print("=" * 70)

    # Ã–rnek veri
    X_train = np.random.rand(1000, 8)
    y_train = np.random.randint(0, 5, 1000)
    X_test = np.random.rand(200, 8)
    y_test = np.random.randint(0, 5, 200)

    class_names = ["DDoS", "SQL Injection", "XSS", "Port Scan", "Brute Force"]

    # Model oluÅŸtur
    model = CyberThreatNeuralNetwork(
        input_dim=8,
        hidden_layers=[256, 128, 64, 32],
        dropout_rate=0.3,
        learning_rate=0.001,
    )

    # EÄŸit
    model.train(
        X_train,
        y_train,
        X_val=X_test,
        y_val=y_test,
        epochs=50,
        batch_size=32,
        class_names=class_names,
    )

    # DeÄŸerlendir
    metrics = model.evaluate(X_test, y_test)

    # GÃ¶rselleÅŸtir
    model.plot_training_history()
    y_pred = model.predict(X_test)
    model.plot_confusion_matrix(y_test, y_pred)

    # Kaydet
    model.save("models/test_model.h5")

    print("\nâœ… Test tamamlandÄ±!")
