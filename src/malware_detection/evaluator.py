"""
Malware Model Evaluator - CyberGuard AI
Model performans deÄŸerlendirmesi

Dosya Yolu: src/malware_detection/evaluator.py
"""

import numpy as np
from typing import Dict, List, Optional
from datetime import datetime

try:
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score,
        confusion_matrix, classification_report, roc_auc_score
    )
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False


class MalwareEvaluator:
    """
    Malware detection model deÄŸerlendirici
    
    Metrikler:
    - Accuracy, Precision, Recall, F1
    - Confusion Matrix
    - ROC-AUC
    - Detection Rate
    """
    
    CLASS_NAMES = ['benign', 'malware']
    
    def __init__(self):
        """Evaluator baÅŸlat"""
        self.evaluation_history: List[Dict] = []
        print("ğŸ“Š Malware Evaluator baÅŸlatÄ±ldÄ±")
    
    def evaluate(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        y_pred_proba: Optional[np.ndarray] = None
    ) -> Dict:
        """
        Model performansÄ±nÄ± deÄŸerlendir
        
        Args:
            y_true: GerÃ§ek etiketler
            y_pred: Tahminler
            y_pred_proba: Tahmin olasÄ±lÄ±klarÄ±
            
        Returns:
            Metrik dictionary
        """
        if not SKLEARN_AVAILABLE:
            raise RuntimeError("scikit-learn yÃ¼klÃ¼ deÄŸil!")
        
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'sample_count': len(y_true)
        }
        
        # Temel metrikler
        metrics['accuracy'] = float(accuracy_score(y_true, y_pred))
        metrics['precision'] = float(precision_score(y_true, y_pred, zero_division=0))
        metrics['recall'] = float(recall_score(y_true, y_pred, zero_division=0))
        metrics['f1_score'] = float(f1_score(y_true, y_pred, zero_division=0))
        
        # Confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        metrics['confusion_matrix'] = cm.tolist()
        
        # TP, TN, FP, FN
        if cm.shape == (2, 2):
            tn, fp, fn, tp = cm.ravel()
            metrics['true_positives'] = int(tp)
            metrics['true_negatives'] = int(tn)
            metrics['false_positives'] = int(fp)
            metrics['false_negatives'] = int(fn)
            
            # Detection rate (sensitivity)
            metrics['detection_rate'] = float(tp / (tp + fn)) if (tp + fn) > 0 else 0.0
            
            # False positive rate
            metrics['false_positive_rate'] = float(fp / (fp + tn)) if (fp + tn) > 0 else 0.0
        
        # ROC-AUC
        if y_pred_proba is not None:
            try:
                metrics['roc_auc'] = float(roc_auc_score(y_true, y_pred_proba))
            except:
                metrics['roc_auc'] = None
        
        # Classification report
        report = classification_report(
            y_true, y_pred,
            target_names=self.CLASS_NAMES,
            output_dict=True,
            zero_division=0
        )
        metrics['classification_report'] = report
        
        # GeÃ§miÅŸe ekle
        self.evaluation_history.append(metrics)
        
        return metrics
    
    def print_report(self, metrics: Dict) -> None:
        """Raporu yazdÄ±r"""
        print("\n" + "=" * 50)
        print("ğŸ“Š MALWARE DETECTION - DEÄERLENDIRME RAPORU")
        print("=" * 50)
        print(f"\nğŸ“ˆ Temel Metrikler:")
        print(f"   Accuracy:       {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
        print(f"   Precision:      {metrics['precision']:.4f}")
        print(f"   Recall:         {metrics['recall']:.4f}")
        print(f"   F1-Score:       {metrics['f1_score']:.4f}")
        
        if 'roc_auc' in metrics and metrics['roc_auc']:
            print(f"   ROC-AUC:        {metrics['roc_auc']:.4f}")
        
        if 'detection_rate' in metrics:
            print(f"\nğŸ¯ Detection Metrics:")
            print(f"   Detection Rate: {metrics['detection_rate']:.4f}")
            print(f"   False Positive: {metrics['false_positive_rate']:.4f}")
        
        print(f"\nğŸ“‹ Confusion Matrix:")
        print(f"   TP: {metrics.get('true_positives', 'N/A')} | FP: {metrics.get('false_positives', 'N/A')}")
        print(f"   FN: {metrics.get('false_negatives', 'N/A')} | TN: {metrics.get('true_negatives', 'N/A')}")
        
        print("=" * 50)
    
    def compare_models(self, evaluations: List[Dict]) -> Dict:
        """Model karÅŸÄ±laÅŸtÄ±rmasÄ± yap"""
        if not evaluations:
            return {}
        
        best_idx = max(range(len(evaluations)), key=lambda i: evaluations[i]['f1_score'])
        
        return {
            'best_model_index': best_idx,
            'best_f1_score': evaluations[best_idx]['f1_score'],
            'comparison': [
                {
                    'index': i,
                    'accuracy': e['accuracy'],
                    'f1_score': e['f1_score']
                }
                for i, e in enumerate(evaluations)
            ]
        }


# Test
if __name__ == "__main__":
    if SKLEARN_AVAILABLE:
        print("ğŸ§ª Malware Evaluator Test\n")
        
        # Mock veri
        np.random.seed(42)
        y_true = np.random.randint(0, 2, 100)
        y_pred = y_true.copy()
        y_pred[:10] = 1 - y_pred[:10]  # %10 hata
        
        # DeÄŸerlendir
        evaluator = MalwareEvaluator()
        metrics = evaluator.evaluate(y_true, y_pred)
        evaluator.print_report(metrics)
    else:
        print("âŒ Test iÃ§in scikit-learn gerekli!")
