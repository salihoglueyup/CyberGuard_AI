"""
Hybrid Optimizer: SSA + Bayesian Optimization
Makale i√ßin geli≈ütirilmi≈ü hiperparametre optimizasyonu

SSA (Salp Swarm Algorithm) + Bayesian Optimization kombinasyonu:
- SSA: Global arama, explorationiyi
- Bayesian: Local arama, exploitation iyi
- Hibrit: ƒ∞kisinin g√º√ßl√º yanlarƒ±nƒ± birle≈ütir

Kullanƒ±m:
    optimizer = HybridOptimizer(objective, search_space)
    best_params, best_score = optimizer.optimize()
"""

import numpy as np
from typing import Dict, Callable, Tuple, List, Any, Optional
from dataclasses import dataclass
import warnings

warnings.filterwarnings("ignore")


@dataclass
class OptimizationResult:
    """Optimizasyon sonucu"""

    best_params: Dict
    best_score: float
    history: List[Dict]
    total_evaluations: int
    convergence_epoch: int


class SSABayesianOptimizer:
    """
    SSA + Bayesian Hibrit Optimizer

    A≈üama 1: SSA ile global arama (exploration)
    A≈üama 2: Bayesian ile local refinement (exploitation)

    Args:
        objective_function: Maximize edilecek fonksiyon
        search_space: Parametre arama alanƒ±
        ssa_iterations: SSA iterasyon sayƒ±sƒ±
        bayesian_iterations: Bayesian iterasyon sayƒ±sƒ±
        population_size: SSA pop√ºlasyon boyutu
        minimize: True ise minimize, False ise maximize
    """

    def __init__(
        self,
        objective_function: Callable[[Dict], float],
        search_space: Dict[str, tuple],
        ssa_iterations: int = 20,
        bayesian_iterations: int = 15,
        population_size: int = 15,
        minimize: bool = False,
        verbose: bool = True,
    ):
        self.objective = objective_function
        self.search_space = search_space
        self.ssa_iterations = ssa_iterations
        self.bayesian_iterations = bayesian_iterations
        self.population_size = population_size
        self.minimize = minimize
        self.verbose = verbose

        self.param_names = list(search_space.keys())
        self.bounds = self._parse_bounds()
        self.history: List[Dict] = []
        self.best_params: Optional[Dict] = None
        self.best_score: float = float("inf") if minimize else float("-inf")

    def _parse_bounds(self) -> List[Tuple[float, float, str]]:
        """Sƒ±nƒ±rlarƒ± parse et"""
        bounds = []
        for name, spec in self.search_space.items():
            lower, upper, dtype = spec
            bounds.append((lower, upper, dtype))
        return bounds

    def _decode_params(self, position: np.ndarray) -> Dict:
        """Pozisyonu parametrelere d√∂n√º≈üt√ºr"""
        params = {}
        for i, (name, (lower, upper, dtype)) in enumerate(
            zip(self.param_names, self.bounds)
        ):
            val = position[i]
            val = np.clip(val, lower, upper)
            if dtype == "int":
                params[name] = int(round(val))
            else:
                params[name] = float(val)
        return params

    def _encode_params(self, params: Dict) -> np.ndarray:
        """Parametreleri pozisyona d√∂n√º≈üt√ºr"""
        position = np.zeros(len(self.param_names))
        for i, name in enumerate(self.param_names):
            position[i] = params.get(name, 0)
        return position

    def _evaluate(self, params: Dict) -> float:
        """Objective function'ƒ± deƒüerlendir"""
        try:
            score = self.objective(params)
            self.history.append({"params": params, "score": score})

            # Best g√ºncelle
            if self.minimize:
                if score < self.best_score:
                    self.best_score = score
                    self.best_params = params.copy()
            else:
                if score > self.best_score:
                    self.best_score = score
                    self.best_params = params.copy()

            return score
        except Exception as e:
            if self.verbose:
                print(f"   ‚ö†Ô∏è Evaluation error: {e}")
            return float("inf") if self.minimize else float("-inf")

    def _ssa_phase(self) -> Tuple[np.ndarray, float]:
        """
        SSA (Salp Swarm Algorithm) fazƒ± - Global exploration
        """
        if self.verbose:
            print("\nü¶† A≈üama 1: SSA Global Arama")
            print("=" * 50)

        dim = len(self.param_names)

        # Pop√ºlasyon ba≈ülat
        population = np.zeros((self.population_size, dim))
        for i in range(self.population_size):
            for j, (lower, upper, _) in enumerate(self.bounds):
                population[i, j] = np.random.uniform(lower, upper)

        # Fitness hesapla
        fitness = np.zeros(self.population_size)
        for i in range(self.population_size):
            params = self._decode_params(population[i])
            fitness[i] = self._evaluate(params)

        # En iyi √ß√∂z√ºm√º bul
        if self.minimize:
            best_idx = np.argmin(fitness)
        else:
            best_idx = np.argmax(fitness)

        food_pos = population[best_idx].copy()
        food_score = fitness[best_idx]

        # SSA iterasyonlarƒ±
        for t in range(self.ssa_iterations):
            c1 = 2 * np.exp(-((4 * t / self.ssa_iterations) ** 2))

            for i in range(self.population_size):
                if i < self.population_size // 2:
                    # Leader salps
                    for j in range(dim):
                        c2 = np.random.random()
                        c3 = np.random.random()
                        lower, upper, _ = self.bounds[j]

                        if c3 < 0.5:
                            population[i, j] = food_pos[j] + c1 * (
                                (upper - lower) * c2 + lower
                            )
                        else:
                            population[i, j] = food_pos[j] - c1 * (
                                (upper - lower) * c2 + lower
                            )
                else:
                    # Follower salps
                    population[i] = (population[i] + population[i - 1]) / 2

                # Sƒ±nƒ±rlarƒ± kontrol et
                for j, (lower, upper, _) in enumerate(self.bounds):
                    population[i, j] = np.clip(population[i, j], lower, upper)

            # Fitness g√ºncelle
            for i in range(self.population_size):
                params = self._decode_params(population[i])
                current_fitness = self._evaluate(params)
                fitness[i] = current_fitness

                # Food g√ºncelle
                if self.minimize:
                    if current_fitness < food_score:
                        food_score = current_fitness
                        food_pos = population[i].copy()
                else:
                    if current_fitness > food_score:
                        food_score = current_fitness
                        food_pos = population[i].copy()

            if self.verbose and (t + 1) % 5 == 0:
                print(
                    f"   SSA Iteration {t+1}/{self.ssa_iterations}: Best = {food_score:.4f}"
                )

        return food_pos, food_score

    def _bayesian_phase(self, initial_point: np.ndarray) -> Tuple[Dict, float]:
        """
        Bayesian Optimization fazƒ± - Local refinement
        Gaussian Process tabanlƒ±
        """
        if self.verbose:
            print("\nüéØ A≈üama 2: Bayesian Local Refinement")
            print("=" * 50)

        try:
            from sklearn.gaussian_process import GaussianProcessRegressor
            from sklearn.gaussian_process.kernels import Matern

            USE_GP = True
        except ImportError:
            USE_GP = False
            if self.verbose:
                print("   ‚ö†Ô∏è sklearn GP bulunamadƒ±, basit local search kullanƒ±lacak")

        # Ba≈ülangƒ±√ß noktalarƒ± (SSA'dan gelen + random)
        X_observed = [initial_point]
        y_observed = [self.best_score if not self.minimize else -self.best_score]

        # SSA history'den ek noktalar al
        for item in self.history[-self.population_size :]:
            X_observed.append(self._encode_params(item["params"]))
            y_observed.append(item["score"] if not self.minimize else -item["score"])

        X_observed = np.array(X_observed)
        y_observed = np.array(y_observed)

        if USE_GP:
            # Gaussian Process ile Bayesian Optimization
            kernel = Matern(nu=2.5)
            gp = GaussianProcessRegressor(
                kernel=kernel, n_restarts_optimizer=5, random_state=42
            )

            for i in range(self.bayesian_iterations):
                # GP'yi fit et
                gp.fit(X_observed, y_observed)

                # Acquisition function ile sonraki noktayƒ± se√ß (UCB)
                best_next = None
                best_acq = float("-inf")

                for _ in range(100):
                    candidate = np.zeros(len(self.param_names))
                    for j, (lower, upper, _) in enumerate(self.bounds):
                        # Ba≈ülangƒ±√ß noktasƒ± etrafƒ±nda arama
                        center = initial_point[j]
                        spread = (upper - lower) * 0.2  # %20 aralƒ±k
                        candidate[j] = np.clip(
                            np.random.normal(center, spread), lower, upper
                        )

                    mu, sigma = gp.predict(candidate.reshape(1, -1), return_std=True)
                    kappa = 2.0
                    acq = mu[0] + kappa * sigma[0]

                    if acq > best_acq:
                        best_acq = acq
                        best_next = candidate

                # Deƒüerlendir
                params = self._decode_params(best_next)
                score = self._evaluate(params)

                X_observed = np.vstack([X_observed, best_next])
                y_observed = np.append(
                    y_observed, score if not self.minimize else -score
                )

                if self.verbose and (i + 1) % 5 == 0:
                    print(
                        f"   Bayesian Iteration {i+1}/{self.bayesian_iterations}: Score = {score:.4f}"
                    )
        else:
            # Basit local search
            for i in range(self.bayesian_iterations):
                candidate = initial_point.copy()
                for j, (lower, upper, _) in enumerate(self.bounds):
                    perturbation = np.random.normal(0, (upper - lower) * 0.1)
                    candidate[j] = np.clip(candidate[j] + perturbation, lower, upper)

                params = self._decode_params(candidate)
                score = self._evaluate(params)

                if (self.minimize and score < self.best_score) or (
                    not self.minimize and score > self.best_score
                ):
                    initial_point = candidate.copy()

        return self.best_params, self.best_score

    def optimize(self) -> OptimizationResult:
        """
        Hibrit optimizasyon √ßalƒ±≈ütƒ±r

        Returns:
            OptimizationResult
        """
        if self.verbose:
            print("\n" + "=" * 60)
            print("üöÄ SSA + Bayesian Hibrit Optimizasyon")
            print("=" * 60)
            print(f"   SSA iterations: {self.ssa_iterations}")
            print(f"   Bayesian iterations: {self.bayesian_iterations}")
            print(f"   Population: {self.population_size}")
            print(f"   Parameters: {self.param_names}")

        # A≈üama 1: SSA
        ssa_best_pos, ssa_best_score = self._ssa_phase()

        if self.verbose:
            print(f"\n   SSA En ƒ∞yi: {self._decode_params(ssa_best_pos)}")
            print(f"   SSA Score: {ssa_best_score:.4f}")

        # A≈üama 2: Bayesian
        final_params, final_score = self._bayesian_phase(ssa_best_pos)

        if self.verbose:
            print(f"\nüèÜ Final En ƒ∞yi Parametreler:")
            for k, v in final_params.items():
                print(f"   {k}: {v}")
            print(f"   Score: {final_score:.4f}")

        # Convergence epoch bul
        convergence = len(self.history)
        for i, item in enumerate(self.history):
            if abs(item["score"] - final_score) < 1e-4:
                convergence = i
                break

        return OptimizationResult(
            best_params=final_params,
            best_score=final_score,
            history=self.history,
            total_evaluations=len(self.history),
            convergence_epoch=convergence,
        )


# Alias
HybridOptimizer = SSABayesianOptimizer


# Test
if __name__ == "__main__":
    print("üß™ SSA + Bayesian Optimizer Test\n")

    # Test objective: Sphere function
    def sphere(params: Dict) -> float:
        x = params.get("x", 0)
        y = params.get("y", 0)
        return -(x**2 + y**2)  # Maximize i√ßin negatif

    search_space = {
        "x": (-5, 5, "float"),
        "y": (-5, 5, "float"),
    }

    optimizer = HybridOptimizer(
        objective_function=sphere,
        search_space=search_space,
        ssa_iterations=10,
        bayesian_iterations=10,
        population_size=10,
        minimize=False,
        verbose=True,
    )

    result = optimizer.optimize()

    print(f"\nüìä Sonu√ß:")
    print(f"   Best: {result.best_params}")
    print(f"   Score: {result.best_score}")
    print(f"   Evaluations: {result.total_evaluations}")
