"""
Ollama Handler - CyberGuard AI
==============================

Local LLM desteÄŸi - Ä°nternet gerektirmez.

Ã–zellikler:
    - Llama 3, Mistral, CodeLlama
    - Tamamen offline Ã§alÄ±ÅŸÄ±r
    - Streaming responses
"""

import os
import logging
from typing import Optional, Dict, List, Any, Generator
from datetime import datetime

try:
    import requests

    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

logger = logging.getLogger("OllamaHandler")


class OllamaHandler:
    """
    Ollama Local LLM Handler

    Desteklenen modeller:
    - llama3.2 (en yeni)
    - llama3.1
    - mistral
    - codellama
    - deepseek-coder
    """

    AVAILABLE_MODELS = {
        "llama3.2": {
            "name": "Llama 3.2",
            "description": "Meta'nÄ±n en yeni modeli",
            "context_window": 128000,
        },
        "llama3.1": {
            "name": "Llama 3.1",
            "description": "Stabil ve gÃ¼Ã§lÃ¼",
            "context_window": 128000,
        },
        "mistral": {
            "name": "Mistral 7B",
            "description": "HÄ±zlÄ± ve verimli",
            "context_window": 32768,
        },
        "codellama": {
            "name": "CodeLlama",
            "description": "Kod yazÄ±mÄ± iÃ§in optimize",
            "context_window": 16384,
        },
        "deepseek-coder": {
            "name": "DeepSeek Coder",
            "description": "Kod ve gÃ¼venlik analizi",
            "context_window": 16384,
        },
    }

    def __init__(
        self,
        model: str = "llama3.2",
        temperature: float = 0.3,
        host: str = "http://localhost:11434",
    ):
        """
        Ollama Handler baÅŸlat

        Args:
            model: KullanÄ±lacak model
            temperature: YaratÄ±cÄ±lÄ±k seviyesi
            host: Ollama server adresi
        """
        self.logger = logging.getLogger("OllamaHandler")

        self.model = model
        self.temperature = temperature
        self.host = host

        self.logger.info(f"âœ… Ollama Handler initialized - Model: {model}")
        print(f"ğŸ¦™ Ollama baÅŸlatÄ±ldÄ± - Model: {model} (local)")

    def chat(
        self,
        user_message: str,
        system_prompt: Optional[str] = None,
        context: Optional[str] = None,
        history: Optional[List[Dict]] = None,
        stream: bool = False,
    ) -> str:
        """
        KullanÄ±cÄ± mesajÄ±na yanÄ±t ver
        """
        try:
            messages = []

            # System prompt
            sys_prompt = system_prompt or self._get_default_system_prompt()
            messages.append({"role": "system", "content": sys_prompt})

            # History
            if history:
                for msg in history[-10:]:
                    messages.append(
                        {
                            "role": msg.get("role", "user"),
                            "content": msg.get("content", ""),
                        }
                    )

            # Context + user message
            full_message = user_message
            if context:
                full_message = f"{context}\n\n---\n\nKullanÄ±cÄ± Sorusu: {user_message}"

            messages.append({"role": "user", "content": full_message})

            self.logger.info(f"ğŸ¦™ Ollama API Ã§aÄŸrÄ±lÄ±yor... ({len(messages)} mesaj)")

            # API request
            url = f"{self.host}/api/chat"
            payload = {
                "model": self.model,
                "messages": messages,
                "stream": stream,
                "options": {
                    "temperature": self.temperature,
                },
            }

            if stream:
                return self._stream_response(url, payload)

            response = requests.post(url, json=payload)

            if response.status_code != 200:
                return f"Ollama hatasÄ±: {response.status_code}"

            result = response.json()
            answer = result.get("message", {}).get("content", "")

            self.logger.info(f"âœ… Ollama yanÄ±t alÄ±ndÄ± ({len(answer)} karakter)")
            return answer

        except requests.exceptions.ConnectionError:
            return "âŒ Ollama sunucusu Ã§alÄ±ÅŸmÄ±yor! `ollama serve` komutu ile baÅŸlatÄ±n."
        except Exception as e:
            self.logger.error(f"âŒ Ollama Error: {e}")
            return f"ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu: {str(e)}"

    def _stream_response(self, url: str, payload: Dict) -> Generator[str, None, None]:
        """Streaming response"""
        try:
            response = requests.post(url, json=payload, stream=True)

            for line in response.iter_lines():
                if line:
                    import json

                    data = json.loads(line)
                    if "message" in data and "content" in data["message"]:
                        yield data["message"]["content"]

        except Exception as e:
            yield f"Hata: {str(e)}"

    def _get_default_system_prompt(self) -> str:
        """VarsayÄ±lan sistem promptu"""
        return f"""Sen CyberGuard AI'Ä±n siber gÃ¼venlik asistanÄ±sÄ±n.
Local olarak Ã§alÄ±ÅŸÄ±yorsun, tamamen offline.

ğŸ¯ GÃ–REVLERÄ°N:
1. Siber gÃ¼venlik sorularÄ±nÄ± yanÄ±tla
2. SaldÄ±rÄ± analizi yap
3. Kod Ã¶rnekleri Ã¼ret
4. Savunma Ã¶nerileri sun

ğŸ“ KURALLAR:
- TÃ¼rkÃ§e yanÄ±t ver
- KÄ±sa ve Ã¶z ol
- Teknik detaylar ver

Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M')}"""

    def list_local_models(self) -> List[str]:
        """Ollama'da yÃ¼klÃ¼ modelleri listele"""
        try:
            response = requests.get(f"{self.host}/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                return [m["name"] for m in models]
        except:
            pass
        return []

    def pull_model(self, model_name: str) -> bool:
        """Model indir"""
        try:
            response = requests.post(f"{self.host}/api/pull", json={"name": model_name})
            return response.status_code == 200
        except:
            return False

    def get_model_info(self) -> Dict:
        """Model bilgilerini dÃ¶ndÃ¼r"""
        model_info = self.AVAILABLE_MODELS.get(self.model, {})
        return {
            "provider": "ollama",
            "model": self.model,
            "name": model_info.get("name", self.model),
            "description": model_info.get("description", "Local model"),
            "context_window": model_info.get("context_window", 0),
            "temperature": self.temperature,
            "host": self.host,
            "is_local": True,
        }

    @classmethod
    def list_models(cls) -> List[Dict]:
        """Mevcut modelleri listele"""
        return [
            {"id": model_id, **info} for model_id, info in cls.AVAILABLE_MODELS.items()
        ]

    @staticmethod
    def is_available() -> bool:
        """Ollama sunucusu Ã§alÄ±ÅŸÄ±yor mu?"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            return response.status_code == 200
        except:
            return False


# Test
if __name__ == "__main__":
    print("ğŸ§ª Ollama Handler Test\n")

    if not OllamaHandler.is_available():
        print("âŒ Ollama sunucusu Ã§alÄ±ÅŸmÄ±yor!")
        print("   `ollama serve` komutu ile baÅŸlatÄ±n.")
    else:
        try:
            handler = OllamaHandler()

            print("ğŸ“‹ Local modeller:")
            for model in handler.list_local_models():
                print(f"   - {model}")

            print("\nğŸ’¬ Test mesajÄ± gÃ¶nderiliyor...")
            response = handler.chat("Merhaba! Kendini tanÄ±t.")
            print(f"\nğŸ¦™ YanÄ±t:\n{response}")

        except Exception as e:
            print(f"âŒ Hata: {e}")
